{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e574330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.2.0.tar.gz (281.3 MB)\n",
      "Collecting py4j==0.10.9.2\n",
      "  Downloading py4j-0.10.9.2-py2.py3-none-any.whl (198 kB)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py): started\n",
      "  Building wheel for pyspark (setup.py): finished with status 'done'\n",
      "  Created wheel for pyspark: filename=pyspark-3.2.0-py2.py3-none-any.whl size=281805913 sha256=5f2938d9184f773b50dc8912b1fe3fdc122661e13e24ad196f5e041d114a3586\n",
      "  Stored in directory: c:\\users\\wish\\appdata\\local\\pip\\cache\\wheels\\23\\f6\\d3\\110e53bd43baeb8d7d38049733d48e39cbecd056f01dba7ee8\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.2 pyspark-3.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Author : Wish MKN\n",
    "#\n",
    "#\n",
    "#pip install pyspark\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df564c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa17e6b3",
   "metadata": {},
   "source": [
    "###### an entry point to PySpark to work with Resilient Distributed Dataset, DataFrame     these are immutable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d8709e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Model_Prediction_Log\").getOrCreate()\n",
    "\n",
    "# entry point to spark sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "221eeb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_interaction_df = spark.read.csv(\"dataframe.csv\", inferSchema = True, header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71e5a85",
   "metadata": {},
   "source": [
    "###### Data imported\n",
    "\n",
    "and displaying schema to get overview about dataset and printing the dataset using .show() and get column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbb05c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- month_interaction_count: integer (nullable = true)\n",
      " |-- week_interaction_count: integer (nullable = true)\n",
      " |-- day_interaction_count: integer (nullable = true)\n",
      " |-- cancelled_within_week: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_interaction_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b98e63b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------------+----------------------+---------------------+---------------------+\n",
      "| user_id|month_interaction_count|week_interaction_count|day_interaction_count|cancelled_within_week|\n",
      "+--------+-----------------------+----------------------+---------------------+---------------------+\n",
      "|66860ae6|                     41|                     9|                    0|                    1|\n",
      "|249803f8|                     25|                     9|                    2|                    0|\n",
      "|32ed74cc|                     21|                     2|                    1|                    1|\n",
      "|7ed76e6a|                     22|                     5|                    2|                    0|\n",
      "|46c81f43|                     32|                     8|                    2|                    0|\n",
      "|cf0f185e|                     26|                     4|                    0|                    1|\n",
      "|568275b3|                     29|                     5|                    1|                    1|\n",
      "|86a060ec|                     33|                     7|                    1|                    1|\n",
      "|c0c07290|                     35|                    10|                    0|                    0|\n",
      "|709dc1da|                     36|                    11|                    1|                    0|\n",
      "+--------+-----------------------+----------------------+---------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_interaction_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5a753b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['user_id',\n",
       " 'month_interaction_count',\n",
       " 'week_interaction_count',\n",
       " 'day_interaction_count',\n",
       " 'cancelled_within_week']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_interaction_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881eac89",
   "metadata": {},
   "source": [
    "#### Features \n",
    "\n",
    "'month_interaction_count',\n",
    " 'week_interaction_count',\n",
    " 'day_interaction_count',\n",
    "\n",
    "#### Label\n",
    "\n",
    "'cancelled_within_week'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91c55be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82daae36",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols = ['month_interaction_count','week_interaction_count','day_interaction_count',\n",
    "                                         'cancelled_within_week'], outputCol = \"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625e34c4",
   "metadata": {},
   "source": [
    "###### Created features using the VectorAssembler and then in final dataset we merge all features column with the cancelled_within_week column which is our label column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ed6045e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = assembler.transform(user_interaction_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c995eb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_interaction_df_final = output.select(\"features\",\"cancelled_within_week\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "802a9394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------------------+\n",
      "|           features|cancelled_within_week|\n",
      "+-------------------+---------------------+\n",
      "| [41.0,9.0,0.0,1.0]|                    1|\n",
      "| [25.0,9.0,2.0,0.0]|                    0|\n",
      "| [21.0,2.0,1.0,1.0]|                    1|\n",
      "| [22.0,5.0,2.0,0.0]|                    0|\n",
      "| [32.0,8.0,2.0,0.0]|                    0|\n",
      "| [26.0,4.0,0.0,1.0]|                    1|\n",
      "| [29.0,5.0,1.0,1.0]|                    1|\n",
      "| [33.0,7.0,1.0,1.0]|                    1|\n",
      "|[35.0,10.0,0.0,0.0]|                    0|\n",
      "|[36.0,11.0,1.0,0.0]|                    0|\n",
      "+-------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_interaction_df_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1fc8abdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = user_interaction_df_final.randomSplit([0.7,0.3], seed = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b26ce88",
   "metadata": {},
   "source": [
    "###### dividing the dataset into train and test dataset by giving 70% as training and 30% as testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d84a5f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732d088b",
   "metadata": {},
   "source": [
    "#### importing the LogisticRegression package and creating LR (logistic regression) model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3d15fea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression_13e6d2b88312"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR = LogisticRegression(labelCol = \"cancelled_within_week\", elasticNetParam=1)\n",
    "LR.setRegParam(0.1)\n",
    "LR.setThreshold(0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249a9ffa",
   "metadata": {},
   "source": [
    "###### LR model created with required below :\n",
    "###### 1. L1 regularization panelty 'elasticNetParam'=1 , the alpha value 1 = L1 penalty and 0 = L2 penalty\n",
    "###### 2. regularization parameter 0.1 , the lamda value\n",
    "###### 3. threshold 0.6, if robability is equal more than 0.6 then the label column will have a value of 1 otherwise 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0eebfa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "LogRModel = LR.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e966544",
   "metadata": {},
   "source": [
    "#####  Train the model on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d41c457e",
   "metadata": {},
   "outputs": [],
   "source": [
    "LogRModel_summary = LogRModel.summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8beb3573",
   "metadata": {},
   "source": [
    "###### assign model summary object to another variable so later we can access it seprately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af579f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wish\\anaconda3\\lib\\site-packages\\pyspark\\sql\\context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------------------+--------------------+--------------------+----------+\n",
      "|           features|cancelled_within_week|       rawPrediction|         probability|prediction|\n",
      "+-------------------+---------------------+--------------------+--------------------+----------+\n",
      "| [21.0,2.0,1.0,1.0]|                  1.0|[-2.0954021468336...|[0.10954451202978...|       1.0|\n",
      "| [22.0,5.0,2.0,0.0]|                  0.0|[2.09540277225602...|[0.89045554897672...|       0.0|\n",
      "| [26.0,4.0,0.0,1.0]|                  1.0|[-2.0954021468336...|[0.10954451202978...|       1.0|\n",
      "| [29.0,5.0,1.0,1.0]|                  1.0|[-2.0954021468336...|[0.10954451202978...|       1.0|\n",
      "| [32.0,8.0,2.0,0.0]|                  0.0|[2.09540277225602...|[0.89045554897672...|       0.0|\n",
      "|[35.0,10.0,0.0,0.0]|                  0.0|[2.09540277225602...|[0.89045554897672...|       0.0|\n",
      "+-------------------+---------------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "LogRModel_summary.predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "95154a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674482d4",
   "metadata": {},
   "source": [
    "##### BinaryClassificationEvaluator as we have values of 0 or 1 if probability equal or greater than 0.6 threshold value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "757e93cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = LogRModel.evaluate(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7e5c9f",
   "metadata": {},
   "source": [
    "###### Model now runs on the test dataset. We compare the cancelled_within_week with prediction\n",
    "\n",
    "compare actual with predicted to get rough idea of how good our model can be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f9ea1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------------------+--------------------+--------------------+----------+\n",
      "|           features|cancelled_within_week|       rawPrediction|         probability|prediction|\n",
      "+-------------------+---------------------+--------------------+--------------------+----------+\n",
      "| [25.0,9.0,2.0,0.0]|                    0|[2.09540277225602...|[0.89045554897672...|       0.0|\n",
      "| [33.0,7.0,1.0,1.0]|                    1|[-2.0954021468336...|[0.10954451202978...|       1.0|\n",
      "|[36.0,11.0,1.0,0.0]|                    0|[2.09540277225602...|[0.89045554897672...|       0.0|\n",
      "| [41.0,9.0,0.0,1.0]|                    1|[-2.0954021468336...|[0.10954451202978...|       1.0|\n",
      "+-------------------+---------------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_labels.predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fd2a4225",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_Res = BinaryClassificationEvaluator(rawPredictionCol = \"prediction\", labelCol = \"cancelled_within_week\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d5f871",
   "metadata": {},
   "source": [
    "##### Evaluating our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "043b0785",
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = evaluation_Res.evaluate(pred_labels.predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "72b9956d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4646d6f",
   "metadata": {},
   "source": [
    "##### get AUC (Area under the curve) value, \n",
    "\n",
    "When AUC = 1, then the classifier is able to perfectly distinguish between all the Positive and the Negative class points correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19287499",
   "metadata": {},
   "source": [
    "####  How Does the AUC-ROC Curve Work?\n",
    "\n",
    "A ROC curve, a higher X-axis value indicates a higher number of False positives than True negatives. While a higher Y-axis value indicates a higher number of True positives than False negatives. So, the choice of the threshold depends on the ability to balance between False positives and False negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2443c114",
   "metadata": {},
   "outputs": [],
   "source": [
    "#<new data with same columns but without cancelled_within_week column> \n",
    "# <new_data_name> = spark.read.csv(\"dataframe.csv\", inferSchema = True, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5d63bb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = LR.fit(user_interaction_df_final)   # set model on complete data set that we have already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ecf863da",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_interaction_df_valid = assembler.transform(user_interaction_df)  \n",
    "\n",
    "# can test for new data set from here which does not have \n",
    "# cancelled withing week column\n",
    "# so instead we put assebler.transform(<new data set after importing with same columns>) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e5b266e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- month_interaction_count: integer (nullable = true)\n",
      " |-- week_interaction_count: integer (nullable = true)\n",
      " |-- day_interaction_count: integer (nullable = true)\n",
      " |-- cancelled_within_week: integer (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_interaction_df_valid.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "945c5fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = final_model.transform(user_interaction_df_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a3263585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+\n",
      "| user_id|prediction|\n",
      "+--------+----------+\n",
      "|66860ae6|       1.0|\n",
      "|249803f8|       0.0|\n",
      "|32ed74cc|       1.0|\n",
      "|7ed76e6a|       0.0|\n",
      "|46c81f43|       0.0|\n",
      "|cf0f185e|       1.0|\n",
      "|568275b3|       1.0|\n",
      "|86a060ec|       1.0|\n",
      "|c0c07290|       0.0|\n",
      "|709dc1da|       0.0|\n",
      "+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res.select('user_id','prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b95fcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
